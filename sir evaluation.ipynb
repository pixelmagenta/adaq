{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin, Doc, Span, Token\n",
    "from spacy.training import Example\n",
    "import verb_cue_classifier\n",
    "import content_classifier\n",
    "import source_classifier\n",
    "import content_resolver\n",
    "import source_resolver\n",
    "import quote_resolver\n",
    "import baseline\n",
    "import spacy\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from spacy.training import offsets_to_biluo_tags\n",
    "\n",
    "\n",
    "from bratcorpus import BratCorpus\n",
    "import spacy_udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_pipeline(nlp, text_features=False, ner=False):\n",
    "    if ner:\n",
    "        ner_vcc = spacy.load(\"verb-cue-classifier/output/model-best\")\n",
    "        nlp.add_pipe(\"ner\", source=ner_vcc, name=\"ner_vcc\", before=\"ner\")\n",
    "    nlp.add_pipe('verb_cue_classifier')\n",
    "    nlp.add_pipe('content_classifier_features')\n",
    "    if text_features:\n",
    "        nlp.add_pipe('content_classifier_text_features')\n",
    "        nlp.add_pipe('content_text_classifier')\n",
    "    else:\n",
    "        nlp.add_pipe('content_classifier')\n",
    "    nlp.add_pipe('source_classifier_features')\n",
    "    if text_features:\n",
    "        nlp.add_pipe('source_classifier_text_features')\n",
    "        nlp.add_pipe('source_text_classifier')\n",
    "    else:\n",
    "        nlp.add_pipe('source_classifier')\n",
    "    nlp.add_pipe('content_resolver')\n",
    "    nlp.add_pipe('source_resolver')\n",
    "    nlp.add_pipe('quote_resolver')\n",
    "    #nlp.add_pipe(\"sentencizer\")\n",
    "    return nlp\n",
    "def span_after_alignment(span, example):\n",
    "    start = example.alignment.x2y[span.start][0]\n",
    "    end = example.alignment.x2y[span.end-1][-1] + 1\n",
    "    return Span(example.reference, start, end)\n",
    "\n",
    "def is_span_in(span, arr):\n",
    "    for other in arr:\n",
    "        if span.start == other.start and span.end == other.end:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_other_span(span, arr):\n",
    "    for other in arr:\n",
    "        if span.start == other.start and span.end == other.end:\n",
    "            return other\n",
    "    return None\n",
    "\n",
    "def f1_bbc(corpus):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    true_count = 0\n",
    "    for ex in corpus:\n",
    "        doc = nlp(ex.predicted)\n",
    "        other = ex.reference\n",
    "        true_count += len(other._.cue_to_content)\n",
    "        for cue, content_spans in doc._.cue_to_content.items():\n",
    "            other_cue = span_after_alignment(cue, ex)\n",
    "            match = True\n",
    "\n",
    "            other_cue = get_other_span(other_cue, other._.verb_cues)\n",
    "            if other_cue is None:\n",
    "                match = False\n",
    "            else:\n",
    "                for content in content_spans:\n",
    "                    other_content = span_after_alignment(content, ex)\n",
    "                    if not is_span_in(other_content, other._.cue_to_content[other_cue]):\n",
    "                        match = False\n",
    "                        break\n",
    "                for source in doc._.cue_to_source[cue]:\n",
    "                    other_source = span_after_alignment(source, ex)\n",
    "                    if not is_span_in(other_source, other._.cue_to_source[other_cue]):\n",
    "                        match = False\n",
    "                        break\n",
    "            if match:\n",
    "                tp += 1\n",
    "            else:\n",
    "                if other_cue is not None:\n",
    "                    print('sentence', doc.text)\n",
    "                    print('pred content', content_spans)\n",
    "                    print('true content', other._.cue_to_content[other_cue])\n",
    "                    print('pred source', doc._.cue_to_source[cue])\n",
    "                    print('true source', other._.cue_to_source[other_cue])\n",
    "                    print('---------------')\n",
    "                fp += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / true_count\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spans_to_label(example, span_f):\n",
    "    label = 'lbl'\n",
    "    \n",
    "    tags_pred = offsets_to_biluo_tags(example.predicted, [(s.start_char, s.end_char, label)\n",
    "                                       for s in span_f(example.predicted)])\n",
    "    tags_pred = ['None' if tag == 'O' else label for tag in tags_pred]\n",
    "    \n",
    "    tags_true = offsets_to_biluo_tags(example.reference, [(s.start_char, s.end_char, label)\n",
    "                                       for s in span_f(example.reference)])\n",
    "    \n",
    "    try:\n",
    "        tags_true = offsets_to_biluo_tags(example.reference, [(s.start_char, s.end_char, label)\n",
    "                                       for s in span_f(example.reference)])\n",
    "    except ValueError:\n",
    "        print(example.reference)\n",
    "        \n",
    "    \n",
    "    tags_true = ['None' if tag == 'O' else label for tag in tags_true]\n",
    "    \n",
    "    tmp = []\n",
    "    for idx, tag in enumerate(tags_pred):\n",
    "        align = example.alignment.x2y[idx]\n",
    "        labels = set([tags_true[i] for i in align])\n",
    "        # t -> [lbl, lbl, None] labels\n",
    "        # if at least one matching tag is labelled, then we consider t labelled\n",
    "        if label in labels:\n",
    "            tmp.append(label)\n",
    "        else:\n",
    "            tmp.append('None')\n",
    "    tags_true = tmp\n",
    "    return tags_pred, tags_true\n",
    "\n",
    "def per_label_metrics(examples):\n",
    "    cue_pred, cue_true = [], []\n",
    "    #content_pred, content_true = [], []\n",
    "    source_pred, source_true = [], []\n",
    "    for ex in examples:\n",
    "        #try:\n",
    "        doc = nlp(ex.predicted)\n",
    "        #print(ex.reference._.path)\n",
    "        #print([(vc, vc.start_char, vc.end_char) for vc in ex.reference._.verb_cues])\n",
    "        new_cue_pred, new_cue_true = spans_to_label(ex, lambda doc: doc._.verb_cues)\n",
    "        cue_pred += new_cue_pred\n",
    "        cue_true += new_cue_true\n",
    "        #new_content_pred, new_content_true = spans_to_label(ex, lambda doc: doc._.content_spans)\n",
    "        #content_pred += new_content_pred\n",
    "        #content_true += new_content_true\n",
    "        new_source_pred, new_source_true = spans_to_label(ex, lambda doc: doc._.source_spans)\n",
    "        source_pred += new_source_pred\n",
    "        source_true += new_source_true\n",
    "        #except Exception as e:\n",
    "        #    print('missed example', e)\n",
    "    \n",
    "    print(\"Source\")\n",
    "    print(classification_report(source_true, source_pred))\n",
    "    \n",
    "    print(\"Cue\")\n",
    "    print(classification_report(cue_true, cue_pred))\n",
    "    \n",
    "    #print(\"Content\")                \n",
    "    #print(classification_report(content_true, content_pred))\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = BratCorpus('sir/data/triple_manual/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/spacy/util.py:1690: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
      "  warnings.warn(Warnings.W111)\n"
     ]
    }
   ],
   "source": [
    "nlp = init_pipeline(spacy_udpipe.load(\"cs\"), text_features=False)\n",
    "#for ex in cs(nlp):\n",
    "#    print(ex)\n",
    "#print(f1_bbc(cs(nlp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch sir/data/triple_manual/doc-5866923.xml.ann informoval | T15\tPHRASE 203 212\tinformova\n",
      "Mismatch sir/data/triple_manual/doc-6706118.xml.ann vydavatelství Redbird Music | T6\tofficial-non-political 1383 1409\tvydavatelství Redbird Musi\n",
      "Mismatch sir/data/triple_manual/doc-7901307.xml.ann Polští hasiči | T4\tanonymous-partial 843 855\tPolští hasič\n",
      "Mismatch sir/data/triple_manual/doc-8404622.xml.ann informoval | T4\tPHRASE 162 171\tinformova\n",
      "Source\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.98      0.85      0.91     14493\n",
      "         lbl       0.15      0.56      0.24       703\n",
      "\n",
      "    accuracy                           0.83     15196\n",
      "   macro avg       0.56      0.70      0.57     15196\n",
      "weighted avg       0.94      0.83      0.88     15196\n",
      "\n",
      "Cue\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.99      1.00      0.99     14926\n",
      "         lbl       0.78      0.48      0.59       270\n",
      "\n",
      "    accuracy                           0.99     15196\n",
      "   macro avg       0.88      0.74      0.79     15196\n",
      "weighted avg       0.99      0.99      0.99     15196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = per_label_metrics(cs(nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/spacy/util.py:1690: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
      "  warnings.warn(Warnings.W111)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.97      0.92      0.94     14493\n",
      "         lbl       0.20      0.42      0.27       703\n",
      "\n",
      "    accuracy                           0.89     15196\n",
      "   macro avg       0.58      0.67      0.60     15196\n",
      "weighted avg       0.93      0.89      0.91     15196\n",
      "\n",
      "Cue\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.99      1.00      0.99     14926\n",
      "         lbl       0.78      0.48      0.59       270\n",
      "\n",
      "    accuracy                           0.99     15196\n",
      "   macro avg       0.88      0.74      0.79     15196\n",
      "weighted avg       0.99      0.99      0.99     15196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = init_pipeline(spacy_udpipe.load(\"cs\"), text_features=True)\n",
    "p = per_label_metrics(cs(nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/spacy/util.py:1690: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
      "  warnings.warn(Warnings.W111)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<baseline.RuleBasedAttribution at 0x7f9fc2268880>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy_udpipe.load(\"cs\")\n",
    "nlp.add_pipe('rule_based_attribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 quotation marks found, indicating an unclosed quotation; given the limitations of this method, it's safest to bail out rather than guess which quotation is unclosed\n",
      "7 quotation marks found, indicating an unclosed quotation; given the limitations of this method, it's safest to bail out rather than guess which quotation is unclosed\n",
      "Source\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.95      1.00      0.98     14493\n",
      "         lbl       0.75      0.00      0.01       703\n",
      "\n",
      "    accuracy                           0.95     15196\n",
      "   macro avg       0.85      0.50      0.49     15196\n",
      "weighted avg       0.94      0.95      0.93     15196\n",
      "\n",
      "Cue\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.98      1.00      0.99     14926\n",
      "         lbl       0.75      0.01      0.02       270\n",
      "\n",
      "    accuracy                           0.98     15196\n",
      "   macro avg       0.87      0.51      0.51     15196\n",
      "weighted avg       0.98      0.98      0.97     15196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = per_label_metrics(cs(nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
