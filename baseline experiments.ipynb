{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Token, DocBin, Doc, Span\n",
    "from spacy.training import Example, offsets_to_biluo_tags\n",
    "import spacy_udpipe\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import baseline\n",
    "from parc3corpus import Parc3Corpus\n",
    "from df_corpus import CsvCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spans_to_label(example, span_f):\n",
    "    label = 'lbl'\n",
    "    \n",
    "    tags_pred = offsets_to_biluo_tags(example.predicted, [(s.start_char, s.end_char, label)\n",
    "                                       for s in span_f(example.predicted)])\n",
    "    tags_pred = ['None' if tag == 'O' else label for tag in tags_pred]\n",
    "    \n",
    "    tags_true = offsets_to_biluo_tags(example.reference, [(s.start_char, s.end_char, label)\n",
    "                                       for s in span_f(example.reference)])\n",
    "    tags_true = ['None' if tag == 'O' else label for tag in tags_true]\n",
    "    \n",
    "    tmp = []\n",
    "    for idx, tag in enumerate(tags_pred):\n",
    "        align = example.alignment.x2y[idx]\n",
    "        labels = set([tags_true[i] for i in align])\n",
    "        # t -> [lbl, lbl, None] labels\n",
    "        # if at least one matching tag is labelled, then we consider t labelled\n",
    "        if label in labels:\n",
    "            tmp.append(label)\n",
    "        else:\n",
    "            tmp.append('None')\n",
    "    tags_true = tmp\n",
    "    return tags_pred, tags_true\n",
    "\n",
    "def per_label_metrics(examples):\n",
    "    cue_pred, cue_true = [], []\n",
    "    content_pred, content_true = [], []\n",
    "    source_pred, source_true = [], []\n",
    "    for ex in examples:\n",
    "        doc = nlp(ex.predicted)\n",
    "        new_cue_pred, new_cue_true = spans_to_label(ex, lambda doc: doc._.verb_cues)\n",
    "        cue_pred += new_cue_pred\n",
    "        cue_true += new_cue_true\n",
    "        new_content_pred, new_content_true = spans_to_label(ex, lambda doc: doc._.content_spans)\n",
    "        content_pred += new_content_pred\n",
    "        content_true += new_content_true\n",
    "        new_source_pred, new_source_true = spans_to_label(ex, lambda doc: doc._.source_spans)\n",
    "        source_pred += new_source_pred\n",
    "        source_true += new_source_true\n",
    "    \n",
    "    print(\"Source\")\n",
    "    print(classification_report(source_true, source_pred))\n",
    "    \n",
    "    print(\"Cue\")\n",
    "    print(classification_report(cue_true, cue_pred))\n",
    "    \n",
    "    print(\"Content\")                \n",
    "    print(classification_report(content_true, content_pred))\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_sentence_labels(examples):\n",
    "\n",
    "    pred_sent_labels = []\n",
    "    true_sent_labels = []\n",
    "\n",
    "    if not Token.has_extension(\"sent_content\"):\n",
    "        Token.set_extension(\"sent_content\", default='None')\n",
    "    for ex in examples:\n",
    "        doc = nlp(ex.predicted)\n",
    "        for content_span in ex.predicted._.content_spans:\n",
    "            for token in content_span:\n",
    "                token._.sent_content = 'content'\n",
    "\n",
    "        for content_span in ex.reference._.content_spans:\n",
    "            for token in content_span:\n",
    "                token._.sent_content = 'content'\n",
    "\n",
    "        for sent in ex.reference.sents:\n",
    "            yes = False\n",
    "            for token in Span(doc,\n",
    "                              ex.alignment.y2x[sent.start][0],\n",
    "                              ex.alignment.y2x[sent.end - 1][-1] + 1):\n",
    "                if token._.sent_content == 'content':\n",
    "                    yes = True\n",
    "                    break\n",
    "            pred_sent_labels.append(yes)\n",
    "\n",
    "        for sent in ex.reference.sents:\n",
    "            yes = False\n",
    "            for token in sent:\n",
    "                if token._.sent_content == 'content':\n",
    "                    yes = True\n",
    "                    break\n",
    "            true_sent_labels.append(yes)\n",
    "    \n",
    "    return pred_sent_labels, true_sent_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = Parc3Corpus('./data/PARC3_complete/test/') #English dataset isn't provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of the English language model and a custom-made pipeline for a baseline rule-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a language model for English\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe('rule_based_attribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric for each element of a quotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.94      1.00      0.97     54277\n",
      "         lbl       0.95      0.13      0.22      3920\n",
      "\n",
      "    accuracy                           0.94     58197\n",
      "   macro avg       0.94      0.56      0.60     58197\n",
      "weighted avg       0.94      0.94      0.92     58197\n",
      "\n",
      "Cue\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.98      1.00      0.99     56916\n",
      "         lbl       0.94      0.23      0.37      1281\n",
      "\n",
      "    accuracy                           0.98     58197\n",
      "   macro avg       0.96      0.61      0.68     58197\n",
      "weighted avg       0.98      0.98      0.98     58197\n",
      "\n",
      "Content\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.74      0.99      0.85     38269\n",
      "         lbl       0.94      0.33      0.49     19928\n",
      "\n",
      "    accuracy                           0.76     58197\n",
      "   macro avg       0.84      0.66      0.67     58197\n",
      "weighted avg       0.81      0.76      0.72     58197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = per_label_metrics(en(nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation of labels for sentence-based metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sent_labels, true_sent_labels = per_sentence_labels(list(en(nlp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the model per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.66      0.97      0.78      1306\n",
      "        True       0.92      0.40      0.56      1110\n",
      "\n",
      "    accuracy                           0.71      2416\n",
      "   macro avg       0.79      0.69      0.67      2416\n",
      "weighted avg       0.78      0.71      0.68      2416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_sent_labels, pred_sent_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Czech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = CsvCorpus(\"./data/okens.csv\", \"./data/cs_sentences.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of the Czech language model and a custom-made pipeline for a baseline rule-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<baseline.RuleBasedAttribution at 0x7f9e8689bcd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy_udpipe.load(\"cs\")\n",
    "nlp.add_pipe('rule_based_attribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric for each element of a quotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 quotation marks found, indicating an unclosed quotation; given the limitations of this method, it's safest to bail out rather than guess which quotation is unclosed\n",
      "Source\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.95      1.00      0.97      4012\n",
      "         lbl       1.00      0.12      0.21       250\n",
      "\n",
      "    accuracy                           0.95      4262\n",
      "   macro avg       0.97      0.56      0.59      4262\n",
      "weighted avg       0.95      0.95      0.93      4262\n",
      "\n",
      "Cue\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.98      1.00      0.99      4137\n",
      "         lbl       1.00      0.23      0.38       125\n",
      "\n",
      "    accuracy                           0.98      4262\n",
      "   macro avg       0.99      0.62      0.68      4262\n",
      "weighted avg       0.98      0.98      0.97      4262\n",
      "\n",
      "Content\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.74      0.99      0.85      2715\n",
      "         lbl       0.97      0.40      0.56      1547\n",
      "\n",
      "    accuracy                           0.78      4262\n",
      "   macro avg       0.86      0.70      0.71      4262\n",
      "weighted avg       0.83      0.78      0.75      4262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = per_label_metrics(cs(nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation of labels for sentence-based metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 quotation marks found, indicating an unclosed quotation; given the limitations of this method, it's safest to bail out rather than guess which quotation is unclosed\n"
     ]
    }
   ],
   "source": [
    "pred_sent_labels, true_sent_labels = per_sentence_labels(list(cs(nlp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the model per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.61      0.96      0.75       108\n",
      "        True       0.91      0.39      0.55       108\n",
      "\n",
      "    accuracy                           0.68       216\n",
      "   macro avg       0.76      0.68      0.65       216\n",
      "weighted avg       0.76      0.68      0.65       216\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_sent_labels, pred_sent_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Russian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru = CsvCorpus(\"./data/ru_tokens.csv\", \"./data/ru_sentences.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of the Russian language model and a custom-made pipeline for a baseline rule-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a language model for Russian\n",
    "# !python -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/spacy/util.py:1690: UserWarning:\n",
      "\n",
      "[W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<baseline.RuleBasedAttribution at 0x7f73310d7be0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "nlp.add_pipe('rule_based_attribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric for each element of a quotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 quotation marks found, indicating an unclosed quotation; given the limitations of this method, it's safest to bail out rather than guess which quotation is unclosed\n",
      "1 quotation marks found, indicating an unclosed quotation; given the limitations of this method, it's safest to bail out rather than guess which quotation is unclosed\n",
      "Source\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.96      1.00      0.98      4217\n",
      "         lbl       0.90      0.18      0.30       208\n",
      "\n",
      "    accuracy                           0.96      4425\n",
      "   macro avg       0.93      0.59      0.64      4425\n",
      "weighted avg       0.96      0.96      0.95      4425\n",
      "\n",
      "Cue\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.98      1.00      0.99      4299\n",
      "         lbl       0.95      0.31      0.47       126\n",
      "\n",
      "    accuracy                           0.98      4425\n",
      "   macro avg       0.97      0.65      0.73      4425\n",
      "weighted avg       0.98      0.98      0.97      4425\n",
      "\n",
      "Content\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.81      0.97      0.88      2920\n",
      "         lbl       0.91      0.55      0.69      1505\n",
      "\n",
      "    accuracy                           0.83      4425\n",
      "   macro avg       0.86      0.76      0.78      4425\n",
      "weighted avg       0.84      0.83      0.82      4425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = per_label_metrics(ru(nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation of labels for sentence-based metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 quotation marks found, indicating an unclosed quotation; given the limitations of this method, it's safest to bail out rather than guess which quotation is unclosed\n",
      "1 quotation marks found, indicating an unclosed quotation; given the limitations of this method, it's safest to bail out rather than guess which quotation is unclosed\n"
     ]
    }
   ],
   "source": [
    "pred_sent_labels, true_sent_labels = per_sentence_labels(list(ru(nlp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the model per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.68      0.93      0.78       108\n",
      "        True       0.87      0.53      0.66       100\n",
      "\n",
      "    accuracy                           0.74       208\n",
      "   macro avg       0.77      0.73      0.72       208\n",
      "weighted avg       0.77      0.74      0.72       208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_sent_labels, pred_sent_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
