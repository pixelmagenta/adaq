{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 00:20:33.374896: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-05 00:20:33.374928: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/pixelmagenta/.local/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2023-04-05 00:20:35.351454: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-05 00:20:35.351476: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-05 00:20:35.351492: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (lordinateur): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Token, DocBin, Doc, Span\n",
    "from spacy.training import Example, offsets_to_biluo_tags\n",
    "import spacy_udpipe\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from parc3corpus import Parc3Corpus\n",
    "from df_corpus import CsvCorpus\n",
    "import verb_cue_classifier\n",
    "import content_classifier\n",
    "import source_classifier\n",
    "import content_resolver\n",
    "import source_resolver\n",
    "import quote_resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_pipeline(nlp, text_features=False, ner=False):\n",
    "    if ner:\n",
    "        ner_vcc = spacy.load(\"verb-cue-classifier/output/model-best\")\n",
    "        nlp.add_pipe(\"ner\", source=ner_vcc, name=\"ner_vcc\", before=\"ner\")\n",
    "    nlp.add_pipe('verb_cue_classifier')\n",
    "    nlp.add_pipe('content_classifier_features')\n",
    "    if text_features:\n",
    "        nlp.add_pipe('content_classifier_text_features')\n",
    "        nlp.add_pipe('content_text_classifier')\n",
    "    else:\n",
    "        nlp.add_pipe('content_classifier')\n",
    "    nlp.add_pipe('source_classifier_features')\n",
    "    if text_features:\n",
    "        nlp.add_pipe('source_classifier_text_features')\n",
    "        nlp.add_pipe('source_text_classifier')\n",
    "    else:\n",
    "        nlp.add_pipe('source_classifier')\n",
    "    nlp.add_pipe('content_resolver')\n",
    "    nlp.add_pipe('source_resolver')\n",
    "    nlp.add_pipe('quote_resolver')\n",
    "    return nlp\n",
    "\n",
    "def span_after_alignment(span, example):\n",
    "    start = example.alignment.x2y[span.start][0]\n",
    "    end = example.alignment.x2y[span.end-1][-1] + 1\n",
    "    return Span(example.reference, start, end)\n",
    "\n",
    "def is_span_in(span, arr):\n",
    "    for other in arr:\n",
    "        if span.start == other.start and span.end == other.end:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_other_span(span, arr):\n",
    "    for other in arr:\n",
    "        if span.start == other.start and span.end == other.end:\n",
    "            return other\n",
    "    return None\n",
    "\n",
    "def exact_matching_metrics(corpus):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    true_count = 0\n",
    "    for ex in corpus:\n",
    "        doc = nlp(ex.predicted)\n",
    "        other = ex.reference\n",
    "        true_count += len(other._.cue_to_content)\n",
    "        for cue, content_spans in doc._.cue_to_content.items():\n",
    "            other_cue = span_after_alignment(cue, ex)\n",
    "            match = True\n",
    "\n",
    "            other_cue = get_other_span(other_cue, other._.verb_cues)\n",
    "            if other_cue is None:\n",
    "                match = False\n",
    "            else:\n",
    "                for content in content_spans:\n",
    "                    other_content = span_after_alignment(content, ex)\n",
    "                    if not is_span_in(other_content, other._.cue_to_content[other_cue]):\n",
    "                        match = False\n",
    "                        break\n",
    "                for source in doc._.cue_to_source[cue]:\n",
    "                    other_source = span_after_alignment(source, ex)\n",
    "                    if not is_span_in(other_source, other._.cue_to_source[other_cue]):\n",
    "                        match = False\n",
    "                        break\n",
    "            if match:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "                \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / true_count\n",
    "    if precision + recall != 0: \n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    else:\n",
    "        f1 = None\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spans_to_label(example, span_f):\n",
    "    label = 'lbl'\n",
    "    \n",
    "    tags_pred = offsets_to_biluo_tags(example.predicted, [(s.start_char, s.end_char, label)\n",
    "                                       for s in span_f(example.predicted)])\n",
    "    tags_pred = ['None' if tag == 'O' else label for tag in tags_pred]\n",
    "    \n",
    "    tags_true = offsets_to_biluo_tags(example.reference, [(s.start_char, s.end_char, label)\n",
    "                                       for s in span_f(example.reference)])\n",
    "    tags_true = ['None' if tag == 'O' else label for tag in tags_true]\n",
    "    \n",
    "    tmp = []\n",
    "    for idx, tag in enumerate(tags_pred):\n",
    "        align = example.alignment.x2y[idx]\n",
    "        labels = set([tags_true[i] for i in align])\n",
    "        # t -> [lbl, lbl, None] labels\n",
    "        # if at least one matching tag is labelled, then we consider t labelled\n",
    "        if label in labels:\n",
    "            tmp.append(label)\n",
    "        else:\n",
    "            tmp.append('None')\n",
    "    tags_true = tmp\n",
    "    return tags_pred, tags_true\n",
    "\n",
    "def per_label_metrics(examples):\n",
    "    cue_pred, cue_true = [], []\n",
    "    content_pred, content_true = [], []\n",
    "    source_pred, source_true = [], []\n",
    "    for ex in examples:\n",
    "        doc = nlp(ex.predicted)\n",
    "        new_cue_pred, new_cue_true = spans_to_label(ex, lambda doc: doc._.verb_cues)\n",
    "        cue_pred += new_cue_pred\n",
    "        cue_true += new_cue_true\n",
    "        new_content_pred, new_content_true = spans_to_label(ex, lambda doc: doc._.content_spans)\n",
    "        content_pred += new_content_pred\n",
    "        content_true += new_content_true\n",
    "        new_source_pred, new_source_true = spans_to_label(ex, lambda doc: doc._.source_spans)\n",
    "        source_pred += new_source_pred\n",
    "        source_true += new_source_true\n",
    "    \n",
    "    print(\"Source\")\n",
    "    print(classification_report(source_true, source_pred))\n",
    "    \n",
    "    print(\"Cue\")\n",
    "    print(classification_report(cue_true, cue_pred))\n",
    "    \n",
    "    print(\"Content\")                \n",
    "    print(classification_report(content_true, content_pred))\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_sentence_labels(examples):\n",
    "\n",
    "    pred_sent_labels = []\n",
    "    true_sent_labels = []\n",
    "\n",
    "    if not Token.has_extension(\"sent_content\"):\n",
    "        Token.set_extension(\"sent_content\", default='None')\n",
    "    for ex in examples:\n",
    "        doc = nlp(ex.predicted)\n",
    "        for content_span in ex.predicted._.content_spans:\n",
    "            for token in content_span:\n",
    "                token._.sent_content = 'content'\n",
    "\n",
    "        for content_span in ex.reference._.content_spans:\n",
    "            for token in content_span:\n",
    "                token._.sent_content = 'content'\n",
    "\n",
    "        for sent in ex.reference.sents:\n",
    "            yes = False\n",
    "            for token in Span(doc,\n",
    "                              ex.alignment.y2x[sent.start][0],\n",
    "                              ex.alignment.y2x[sent.end - 1][-1] + 1):\n",
    "                if token._.sent_content == 'content':\n",
    "                    yes = True\n",
    "                    break\n",
    "            pred_sent_labels.append(yes)\n",
    "\n",
    "        for sent in ex.reference.sents:\n",
    "            yes = False\n",
    "            for token in sent:\n",
    "                if token._.sent_content == 'content':\n",
    "                    yes = True\n",
    "                    break\n",
    "            true_sent_labels.append(yes)\n",
    "    \n",
    "    return pred_sent_labels, true_sent_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Czech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = CsvCorpus(\"./data/cs_tokens.csv\", \"./data/cs_sentences.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of the system trained without text features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of the Czech language model and initialization of a pipeline for a system without text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.0, None)\n"
     ]
    }
   ],
   "source": [
    "nlp = init_pipeline(spacy_udpipe.load(\"cs\"), text_features=False)\n",
    "print(exact_matching_metrics(cs(nlp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric for each element of a quotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       1.00      0.71      0.83      4012\n",
      "         lbl       0.17      0.96      0.29       250\n",
      "\n",
      "    accuracy                           0.73      4262\n",
      "   macro avg       0.58      0.84      0.56      4262\n",
      "weighted avg       0.95      0.73      0.80      4262\n",
      "\n",
      "Cue\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       1.00      1.00      1.00      4137\n",
      "         lbl       0.92      0.84      0.88       125\n",
      "\n",
      "    accuracy                           0.99      4262\n",
      "   macro avg       0.96      0.92      0.94      4262\n",
      "weighted avg       0.99      0.99      0.99      4262\n",
      "\n",
      "Content\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.92      0.43      0.59      2715\n",
      "         lbl       0.48      0.94      0.64      1547\n",
      "\n",
      "    accuracy                           0.61      4262\n",
      "   macro avg       0.70      0.68      0.61      4262\n",
      "weighted avg       0.76      0.61      0.60      4262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = per_label_metrics(cs(nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation of labels for sentence-based metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sent_labels, true_sent_labels = per_sentence_labels(cs(nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the model per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.59      0.74       108\n",
      "        True       0.71      1.00      0.83       108\n",
      "\n",
      "    accuracy                           0.80       216\n",
      "   macro avg       0.86      0.80      0.79       216\n",
      "weighted avg       0.86      0.80      0.79       216\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_sent_labels, pred_sent_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of the system trained with text features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of the Czech language model and initialization of a pipeline for a system with text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.017699115044247787, 0.018018018018018018, 0.01785714285714286)\n"
     ]
    }
   ],
   "source": [
    "nlp = init_pipeline(spacy_udpipe.load(\"cs\"), text_features=True)\n",
    "print(exact_matching_metrics(cs(nlp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.98      0.94      0.96      4012\n",
      "         lbl       0.40      0.68      0.50       250\n",
      "\n",
      "    accuracy                           0.92      4262\n",
      "   macro avg       0.69      0.81      0.73      4262\n",
      "weighted avg       0.94      0.92      0.93      4262\n",
      "\n",
      "Cue\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       1.00      1.00      1.00      4137\n",
      "         lbl       0.92      0.84      0.88       125\n",
      "\n",
      "    accuracy                           0.99      4262\n",
      "   macro avg       0.96      0.92      0.94      4262\n",
      "weighted avg       0.99      0.99      0.99      4262\n",
      "\n",
      "Content\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.99      0.65      0.79      2715\n",
      "         lbl       0.62      0.99      0.76      1547\n",
      "\n",
      "    accuracy                           0.78      4262\n",
      "   macro avg       0.80      0.82      0.77      4262\n",
      "weighted avg       0.86      0.78      0.78      4262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = per_label_metrics(cs(nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation of labels for sentence-based metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sent_labels, true_sent_labels = per_sentence_labels(cs(nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the model per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.88      0.94       108\n",
      "        True       0.89      1.00      0.94       108\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_sent_labels, pred_sent_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru = CsvCorpus(\"./data/ru_tokens.csv\", \"./data/ru_sentences.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of the system trained without text features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of the Russian language model and initialization of a pipeline for a system without text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a language model for Russian\n",
    "# !python -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.0, None)\n"
     ]
    }
   ],
   "source": [
    "nlp = init_pipeline(spacy.load(\"ru_core_news_sm\"), text_features=False)\n",
    "print(exact_matching_metrics(ru(nlp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.99      0.89      0.94      4217\n",
      "         lbl       0.28      0.84      0.42       208\n",
      "\n",
      "    accuracy                           0.89      4425\n",
      "   macro avg       0.64      0.87      0.68      4425\n",
      "weighted avg       0.96      0.89      0.92      4425\n",
      "\n",
      "Cue\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.99      1.00      0.99      4299\n",
      "         lbl       0.81      0.71      0.75       126\n",
      "\n",
      "    accuracy                           0.99      4425\n",
      "   macro avg       0.90      0.85      0.87      4425\n",
      "weighted avg       0.99      0.99      0.99      4425\n",
      "\n",
      "Content\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.87      0.87      0.87      2920\n",
      "         lbl       0.74      0.74      0.74      1505\n",
      "\n",
      "    accuracy                           0.82      4425\n",
      "   macro avg       0.80      0.80      0.80      4425\n",
      "weighted avg       0.82      0.82      0.82      4425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = per_label_metrics(ru(nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sent_labels, true_sent_labels = per_sentence_labels(ru(nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      0.95      0.90       108\n",
      "        True       0.94      0.81      0.87       100\n",
      "\n",
      "    accuracy                           0.88       208\n",
      "   macro avg       0.89      0.88      0.88       208\n",
      "weighted avg       0.89      0.88      0.88       208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_sent_labels, pred_sent_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of the system trained with text features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of the Russian language model and initialization of a pipeline for a system with text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.0, None)\n"
     ]
    }
   ],
   "source": [
    "nlp = init_pipeline(spacy.load(\"ru_core_news_sm\"), text_features=True)\n",
    "print(exact_matching_metrics(ru(nlp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.98      0.95      0.97      4217\n",
      "         lbl       0.40      0.65      0.50       208\n",
      "\n",
      "    accuracy                           0.94      4425\n",
      "   macro avg       0.69      0.80      0.73      4425\n",
      "weighted avg       0.95      0.94      0.94      4425\n",
      "\n",
      "Cue\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.99      1.00      0.99      4299\n",
      "         lbl       0.81      0.71      0.75       126\n",
      "\n",
      "    accuracy                           0.99      4425\n",
      "   macro avg       0.90      0.85      0.87      4425\n",
      "weighted avg       0.99      0.99      0.99      4425\n",
      "\n",
      "Content\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.96      0.78      0.86      2920\n",
      "         lbl       0.68      0.94      0.79      1505\n",
      "\n",
      "    accuracy                           0.83      4425\n",
      "   macro avg       0.82      0.86      0.82      4425\n",
      "weighted avg       0.87      0.83      0.84      4425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = per_label_metrics(ru(nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sent_labels, true_sent_labels = per_sentence_labels(ru(nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.94      0.94       108\n",
      "        True       0.93      0.95      0.94       100\n",
      "\n",
      "    accuracy                           0.94       208\n",
      "   macro avg       0.94      0.94      0.94       208\n",
      "weighted avg       0.94      0.94      0.94       208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_sent_labels, pred_sent_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = Parc3Corpus('./data/PARC3_complete/test/') #English dataset isn't provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of the system trained without text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pixelmagenta/.local/lib/python3.10/site-packages/spacy/util.py:877: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.5 and may not be 100% compatible with the current version (3.4.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4541176470588235, 0.37658536585365854, 0.41173333333333334)\n"
     ]
    }
   ],
   "source": [
    "nlp = init_pipeline(spacy.load(\"en_core_web_sm\"), text_features=False, ner=True)\n",
    "print(exact_matching_metrics(en(nlp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.98      0.98      0.98     54277\n",
      "         lbl       0.77      0.71      0.74      3920\n",
      "\n",
      "    accuracy                           0.97     58197\n",
      "   macro avg       0.87      0.85      0.86     58197\n",
      "weighted avg       0.97      0.97      0.97     58197\n",
      "\n",
      "Cue\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.99      1.00      1.00     56916\n",
      "         lbl       0.84      0.72      0.78      1281\n",
      "\n",
      "    accuracy                           0.99     58197\n",
      "   macro avg       0.92      0.86      0.89     58197\n",
      "weighted avg       0.99      0.99      0.99     58197\n",
      "\n",
      "Content\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.89      0.94      0.91     38269\n",
      "         lbl       0.87      0.78      0.82     19928\n",
      "\n",
      "    accuracy                           0.88     58197\n",
      "   macro avg       0.88      0.86      0.87     58197\n",
      "weighted avg       0.88      0.88      0.88     58197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = per_label_metrics(en(nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sent_labels, true_sent_labels = per_sentence_labels(en(nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.82      0.95      0.88      1306\n",
      "        True       0.93      0.75      0.83      1110\n",
      "\n",
      "    accuracy                           0.86      2416\n",
      "   macro avg       0.88      0.85      0.86      2416\n",
      "weighted avg       0.87      0.86      0.86      2416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_sent_labels, pred_sent_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of the system trained with text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pixelmagenta/.local/lib/python3.10/site-packages/spacy/util.py:877: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.5 and may not be 100% compatible with the current version (3.4.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4517593643586833, 0.3882926829268293, 0.4176285414480587)\n"
     ]
    }
   ],
   "source": [
    "nlp = init_pipeline(spacy.load(\"en_core_web_sm\"), text_features=True, ner=True)\n",
    "print(exact_matching_metrics(en(nlp)))\n",
    "#print(f1_bbc(en(nlp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.98      0.99      0.98     54277\n",
      "         lbl       0.80      0.77      0.78      3920\n",
      "\n",
      "    accuracy                           0.97     58197\n",
      "   macro avg       0.89      0.88      0.88     58197\n",
      "weighted avg       0.97      0.97      0.97     58197\n",
      "\n",
      "Cue\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.99      1.00      1.00     56916\n",
      "         lbl       0.84      0.72      0.78      1281\n",
      "\n",
      "    accuracy                           0.99     58197\n",
      "   macro avg       0.92      0.86      0.89     58197\n",
      "weighted avg       0.99      0.99      0.99     58197\n",
      "\n",
      "Content\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.89      0.94      0.91     38269\n",
      "         lbl       0.87      0.78      0.82     19928\n",
      "\n",
      "    accuracy                           0.89     58197\n",
      "   macro avg       0.88      0.86      0.87     58197\n",
      "weighted avg       0.88      0.89      0.88     58197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = per_label_metrics(en(nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sent_labels, true_sent_labels = per_sentence_labels(en(nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.83      0.95      0.89      1306\n",
      "        True       0.93      0.77      0.85      1110\n",
      "\n",
      "    accuracy                           0.87      2416\n",
      "   macro avg       0.88      0.86      0.87      2416\n",
      "weighted avg       0.88      0.87      0.87      2416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_sent_labels, pred_sent_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
